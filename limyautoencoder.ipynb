{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build auto-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "encoding_dim = 200  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_seq = Input(shape=(5000,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "#encoded = Dense(encoding_dim, activation='relu')(input_seq)\n",
    "encoded = Dense(1024, activation='relu')(input_seq)\n",
    "encoded = Dense(256, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(1024, activation='relu')(decoded)\n",
    "decoded = Dense(5000, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input=input_seq, output=decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input=input_seq, output=encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "#decoder_layers = autoencoder.layers[-5:-1]\n",
    "# create the decoder model\n",
    "#decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(875, 5000)\n",
      "(879, 5000)\n",
      "(1600, 5000)\n",
      "(154, 5000)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nrrd\n",
    "import glob\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "posintpaths= 'postrainsamples/postotalintensity.save'\n",
    "negintpaths= 'negtrainsamples/negtotalintensity.save'\n",
    "\n",
    "\n",
    "postotalintensity = np.load(posintpaths)\n",
    "negtotalintensity = np.load(negintpaths)\n",
    "#datapath = os.path.expanduser(\"~\")+'/DATA/Case  '\n",
    "print(postotalintensity.shape)\n",
    "print(negtotalintensity.shape)\n",
    "\n",
    "traindata=np.concatenate((postotalintensity[:800],negtotalintensity[:800]),axis=0)\n",
    "testdata=np.concatenate((postotalintensity[800:],negtotalintensity[800:]),axis=0)\n",
    "for i in range(len(traindata)):\n",
    "    for j in range(5000):\n",
    "        if traindata[i][j]>255:\n",
    "            traindata[i][j]=255\n",
    "for i in range(len(testdata)):\n",
    "    for j in range(5000):\n",
    "        if testdata[i][j]>255:\n",
    "            testdata[i][j]=255\n",
    "#normalize data to value between 0 to 1            \n",
    "traindata=traindata.astype('float32') / 255.\n",
    "testdata=testdata.astype('float32') / 255.\n",
    "\n",
    "print(traindata.shape)\n",
    "print(testdata.shape)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 151 samples\n",
      "Epoch 1/200\n",
      "1600/1600 [==============================] - 1s - loss: 0.6931 - val_loss: 0.6930\n",
      "Epoch 2/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6928 - val_loss: 0.6926\n",
      "Epoch 4/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6926 - val_loss: 0.6924\n",
      "Epoch 5/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6924 - val_loss: 0.6922\n",
      "Epoch 6/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6923 - val_loss: 0.6920\n",
      "Epoch 7/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6921 - val_loss: 0.6918\n",
      "Epoch 8/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6919 - val_loss: 0.6916\n",
      "Epoch 9/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6918 - val_loss: 0.6914\n",
      "Epoch 10/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6916 - val_loss: 0.6912\n",
      "Epoch 11/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6914 - val_loss: 0.6910\n",
      "Epoch 12/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6913 - val_loss: 0.6908\n",
      "Epoch 13/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6911 - val_loss: 0.6906\n",
      "Epoch 14/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6909 - val_loss: 0.6904\n",
      "Epoch 15/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6907 - val_loss: 0.6901\n",
      "Epoch 16/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6905 - val_loss: 0.6899\n",
      "Epoch 17/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6903 - val_loss: 0.6897\n",
      "Epoch 18/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6901 - val_loss: 0.6894\n",
      "Epoch 19/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6899 - val_loss: 0.6892\n",
      "Epoch 20/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6897 - val_loss: 0.6890\n",
      "Epoch 21/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6895 - val_loss: 0.6887\n",
      "Epoch 22/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6893 - val_loss: 0.6884\n",
      "Epoch 23/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6891 - val_loss: 0.6881\n",
      "Epoch 24/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6888 - val_loss: 0.6878\n",
      "Epoch 25/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6886 - val_loss: 0.6875\n",
      "Epoch 26/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6883 - val_loss: 0.6872\n",
      "Epoch 27/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6880 - val_loss: 0.6868\n",
      "Epoch 28/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6877 - val_loss: 0.6864\n",
      "Epoch 29/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6874 - val_loss: 0.6860\n",
      "Epoch 30/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6870 - val_loss: 0.6855\n",
      "Epoch 31/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6867 - val_loss: 0.6850\n",
      "Epoch 32/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6862 - val_loss: 0.6844\n",
      "Epoch 33/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6857 - val_loss: 0.6837\n",
      "Epoch 34/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6851 - val_loss: 0.6828\n",
      "Epoch 35/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6843 - val_loss: 0.6817\n",
      "Epoch 36/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6834 - val_loss: 0.6803\n",
      "Epoch 37/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6822 - val_loss: 0.6781\n",
      "Epoch 38/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6805 - val_loss: 0.6751\n",
      "Epoch 39/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6780 - val_loss: 0.6714\n",
      "Epoch 40/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6742 - val_loss: 0.6644\n",
      "Epoch 41/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6675 - val_loss: 0.6489\n",
      "Epoch 42/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6545 - val_loss: 0.6246\n",
      "Epoch 43/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.6282 - val_loss: 0.5686\n",
      "Epoch 44/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.5873 - val_loss: 0.5102\n",
      "Epoch 45/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.5226 - val_loss: 0.4725\n",
      "Epoch 46/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.5047 - val_loss: 0.3869\n",
      "Epoch 47/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.4664 - val_loss: 0.3710\n",
      "Epoch 48/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3916 - val_loss: 0.3630\n",
      "Epoch 49/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3651 - val_loss: 0.3549\n",
      "Epoch 50/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3601 - val_loss: 0.3503\n",
      "Epoch 51/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3570 - val_loss: 0.3511\n",
      "Epoch 52/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3590 - val_loss: 0.3500\n",
      "Epoch 53/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3553 - val_loss: 0.3465\n",
      "Epoch 54/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3518 - val_loss: 0.3490\n",
      "Epoch 55/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3534 - val_loss: 0.3455\n",
      "Epoch 56/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3529 - val_loss: 0.3441\n",
      "Epoch 57/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3499 - val_loss: 0.3447\n",
      "Epoch 58/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3517 - val_loss: 0.3479\n",
      "Epoch 59/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3507 - val_loss: 0.3486\n",
      "Epoch 60/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3514 - val_loss: 0.3439\n",
      "Epoch 61/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3493 - val_loss: 0.3432\n",
      "Epoch 62/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3486 - val_loss: 0.3427\n",
      "Epoch 63/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3488 - val_loss: 0.3431\n",
      "Epoch 64/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3497 - val_loss: 0.3449\n",
      "Epoch 65/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3499 - val_loss: 0.3440\n",
      "Epoch 66/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3494 - val_loss: 0.3428\n",
      "Epoch 67/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3482 - val_loss: 0.3423\n",
      "Epoch 68/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3480 - val_loss: 0.3423\n",
      "Epoch 69/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3480 - val_loss: 0.3425\n",
      "Epoch 70/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3484 - val_loss: 0.3455\n",
      "Epoch 71/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3496 - val_loss: 0.3440\n",
      "Epoch 72/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3489 - val_loss: 0.3429\n",
      "Epoch 73/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3480 - val_loss: 0.3423\n",
      "Epoch 74/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3477 - val_loss: 0.3421\n",
      "Epoch 75/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3477 - val_loss: 0.3424\n",
      "Epoch 76/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3479 - val_loss: 0.3428\n",
      "Epoch 77/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3481 - val_loss: 0.3428\n",
      "Epoch 78/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3491 - val_loss: 0.3421\n",
      "Epoch 79/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3421\n",
      "Epoch 80/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3423\n",
      "Epoch 81/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3420\n",
      "Epoch 82/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3474 - val_loss: 0.3421\n",
      "Epoch 83/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3476 - val_loss: 0.3441\n",
      "Epoch 84/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3487 - val_loss: 0.3423\n",
      "Epoch 85/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3473 - val_loss: 0.3420\n",
      "Epoch 86/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3419\n",
      "Epoch 87/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3421\n",
      "Epoch 88/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3474 - val_loss: 0.3438\n",
      "Epoch 89/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3479 - val_loss: 0.3434\n",
      "Epoch 90/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3478 - val_loss: 0.3430\n",
      "Epoch 91/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3476 - val_loss: 0.3428\n",
      "Epoch 92/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3476 - val_loss: 0.3423\n",
      "Epoch 93/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3419\n",
      "Epoch 94/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3418\n",
      "Epoch 95/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3418\n",
      "Epoch 96/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3419\n",
      "Epoch 97/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3421\n",
      "Epoch 98/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3442\n",
      "Epoch 99/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3485 - val_loss: 0.3443\n",
      "Epoch 100/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3420\n",
      "Epoch 101/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3419\n",
      "Epoch 102/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3470 - val_loss: 0.3418\n",
      "Epoch 103/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3418\n",
      "Epoch 104/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3418\n",
      "Epoch 105/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3418\n",
      "Epoch 106/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3419\n",
      "Epoch 107/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3470 - val_loss: 0.3422\n",
      "Epoch 108/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3470 - val_loss: 0.3431\n",
      "Epoch 109/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3480 - val_loss: 0.3425\n",
      "Epoch 110/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3473 - val_loss: 0.3421\n",
      "Epoch 111/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3418\n",
      "Epoch 112/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3468 - val_loss: 0.3419\n",
      "Epoch 113/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3468 - val_loss: 0.3420\n",
      "Epoch 114/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3420\n",
      "Epoch 115/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3469 - val_loss: 0.3423\n",
      "Epoch 116/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3472 - val_loss: 0.3423\n",
      "Epoch 117/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3470 - val_loss: 0.3434\n",
      "Epoch 118/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3429\n",
      "Epoch 119/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3473 - val_loss: 0.3430\n",
      "Epoch 120/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3474 - val_loss: 0.3435\n",
      "Epoch 121/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3420\n",
      "Epoch 122/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3467 - val_loss: 0.3418\n",
      "Epoch 123/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3466 - val_loss: 0.3418\n",
      "Epoch 124/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3465 - val_loss: 0.3417\n",
      "Epoch 125/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3465 - val_loss: 0.3418\n",
      "Epoch 126/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3465 - val_loss: 0.3418\n",
      "Epoch 127/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3465 - val_loss: 0.3428\n",
      "Epoch 128/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3470 - val_loss: 0.3424\n",
      "Epoch 129/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3466 - val_loss: 0.3425\n",
      "Epoch 130/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3474 - val_loss: 0.3424\n",
      "Epoch 131/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3468 - val_loss: 0.3422\n",
      "Epoch 132/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3465 - val_loss: 0.3423\n",
      "Epoch 133/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3464 - val_loss: 0.3428\n",
      "Epoch 134/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3473 - val_loss: 0.3480\n",
      "Epoch 135/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3479 - val_loss: 0.3446\n",
      "Epoch 136/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3471 - val_loss: 0.3423\n",
      "Epoch 137/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3458 - val_loss: 0.3419\n",
      "Epoch 138/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3458 - val_loss: 0.3423\n",
      "Epoch 139/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3460 - val_loss: 0.3429\n",
      "Epoch 140/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3466 - val_loss: 0.3464\n",
      "Epoch 141/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3462 - val_loss: 0.3426\n",
      "Epoch 142/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3462 - val_loss: 0.3440\n",
      "Epoch 143/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3453 - val_loss: 0.3427\n",
      "Epoch 144/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3449 - val_loss: 0.3488\n",
      "Epoch 145/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3467 - val_loss: 0.3423\n",
      "Epoch 146/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3439 - val_loss: 0.3420\n",
      "Epoch 147/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3435 - val_loss: 0.3448\n",
      "Epoch 148/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3474 - val_loss: 0.3494\n",
      "Epoch 149/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3462 - val_loss: 0.3440\n",
      "Epoch 150/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3429 - val_loss: 0.3420\n",
      "Epoch 151/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3412 - val_loss: 0.3421\n",
      "Epoch 152/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3424 - val_loss: 0.3910\n",
      "Epoch 153/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3456 - val_loss: 0.3421\n",
      "Epoch 154/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3386 - val_loss: 0.3432\n",
      "Epoch 155/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3400 - val_loss: 0.3426\n",
      "Epoch 156/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3386 - val_loss: 0.3457\n",
      "Epoch 157/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3378 - val_loss: 0.3427\n",
      "Epoch 158/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3362 - val_loss: 0.3485\n",
      "Epoch 159/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3373 - val_loss: 0.3424\n",
      "Epoch 160/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3332 - val_loss: 0.3417\n",
      "Epoch 161/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3363 - val_loss: 0.3495\n",
      "Epoch 162/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3382 - val_loss: 0.3421\n",
      "Epoch 163/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3313 - val_loss: 0.3425\n",
      "Epoch 164/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3343 - val_loss: 0.3442\n",
      "Epoch 165/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3302 - val_loss: 0.3413\n",
      "Epoch 166/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3335 - val_loss: 0.3419\n",
      "Epoch 167/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3291 - val_loss: 0.3424\n",
      "Epoch 168/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3343 - val_loss: 0.3432\n",
      "Epoch 169/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3300 - val_loss: 0.3420\n",
      "Epoch 170/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3279 - val_loss: 0.3462\n",
      "Epoch 171/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3303 - val_loss: 0.3413\n",
      "Epoch 172/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3279 - val_loss: 0.3445\n",
      "Epoch 173/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3289 - val_loss: 0.3415\n",
      "Epoch 174/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3310 - val_loss: 0.3416\n",
      "Epoch 175/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3268 - val_loss: 0.3408\n",
      "Epoch 176/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3298 - val_loss: 0.3437\n",
      "Epoch 177/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3282 - val_loss: 0.3409\n",
      "Epoch 178/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3273 - val_loss: 0.3423\n",
      "Epoch 179/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3287 - val_loss: 0.3416\n",
      "Epoch 180/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3274 - val_loss: 0.3418\n",
      "Epoch 181/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3290 - val_loss: 0.3426\n",
      "Epoch 182/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3276 - val_loss: 0.3413\n",
      "Epoch 183/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3271 - val_loss: 0.3408\n",
      "Epoch 184/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3273 - val_loss: 0.3422\n",
      "Epoch 185/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3279 - val_loss: 0.3405\n",
      "Epoch 186/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3271 - val_loss: 0.3404\n",
      "Epoch 187/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3257 - val_loss: 0.3407\n",
      "Epoch 188/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3277 - val_loss: 0.3399\n",
      "Epoch 189/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3281 - val_loss: 0.3406\n",
      "Epoch 190/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3274 - val_loss: 0.3464\n",
      "Epoch 191/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3264 - val_loss: 0.3413\n",
      "Epoch 192/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3265 - val_loss: 0.3395\n",
      "Epoch 193/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3245 - val_loss: 0.3418\n",
      "Epoch 194/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3279 - val_loss: 0.3439\n",
      "Epoch 195/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3273 - val_loss: 0.3400\n",
      "Epoch 196/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3251 - val_loss: 0.3424\n",
      "Epoch 197/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3278 - val_loss: 0.3401\n",
      "Epoch 198/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3248 - val_loss: 0.3414\n",
      "Epoch 199/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3270 - val_loss: 0.3442\n",
      "Epoch 200/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3293 - val_loss: 0.3397\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(traindata, traindata,\n",
    "                nb_epoch=200,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(testdata, testdata))\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 32)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#get encoder result\n",
    "encoded_traindata = encoder.predict(traindata)\n",
    "encoded_testdata = encoder.predict(testdata)\n",
    "print(encoded_traindata.shape)\n",
    "#make label for train and test data\n",
    "poslabel=np.ones((800,), dtype=np.int)\n",
    "neglabel=np.zeros((800,), dtype=np.int)\n",
    "trainlabel=np.hstack((poslabel,neglabel))\n",
    "poslabel=np.ones((72,), dtype=np.int)\n",
    "neglabel=np.zeros((79,), dtype=np.int)\n",
    "testlabel=np.hstack((poslabel,neglabel))\n",
    "#build many kinds of classifier\n",
    "svmclf = svm.SVC(kernel=\"rbf\")\n",
    "svmclf = svmclf.fit(encoded_traindata, trainlabel) \n",
    "\n",
    "rfclf = RandomForestClassifier(n_estimators=20)\n",
    "rfclf = rfclf.fit(encoded_traindata, trainlabel)\n",
    "\n",
    "abclf = AdaBoostClassifier(n_estimators=100)\n",
    "abclf = abclf.fit(encoded_traindata, trainlabel)\n",
    "\n",
    "gbclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "gbclf = gbclf.fit(encoded_traindata, trainlabel)\n",
    "\n",
    "bgclf = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n",
    "bgclf = bgclf.fit(encoded_traindata, trainlabel)\n",
    "\n",
    "gnbclf = GaussianNB()\n",
    "gnbclf = gnbclf.fit(encoded_traindata, trainlabel)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict testdata ,give result and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 32)\n",
      "0.960264900662 0.920529801325 0.913907284768\n",
      "Based on intensity with autoencoder---------------\n",
      "random forest: 0.9722222222222222 0.9493670886075949 0.9602649006622517\n",
      "adaboost: 0.9722222222222222 0.8734177215189873 0.9205298013245033\n",
      "gradient boost: 0.9305555555555556 0.8987341772151899 0.9139072847682119\n",
      "svm: 0.9722222222222222 0.9873417721518988 0.9801324503311258\n",
      "bagging: 0.9583333333333334 0.9873417721518988 0.9735099337748344\n",
      "gaussian naive bayes: 0.9861111111111112 0.6075949367088608 0.7880794701986755\n"
     ]
    }
   ],
   "source": [
    "#encode testdata\n",
    "encoded_testdata = encoder.predict(testdata)\n",
    "print(encoded_traindata.shape)\n",
    "#predict testdata\n",
    "rfresult=rfclf.predict(encoded_testdata)\n",
    "abresult=abclf.predict(encoded_testdata)\n",
    "gbresult=gbclf.predict(encoded_testdata)\n",
    "svmresult=svmclf.predict(encoded_testdata)\n",
    "bgresult=bgclf.predict(encoded_testdata)\n",
    "gnbresult=gnbclf.predict(encoded_testdata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#calculate pos,neg,total accuracy\n",
    "rfposright=0\n",
    "rfnegright=0\n",
    "abposright=0\n",
    "abnegright=0\n",
    "gbnegright=0\n",
    "gbposright=0\n",
    "svmposright=0\n",
    "svmnegright=0\n",
    "bgposright=0\n",
    "bgnegright=0\n",
    "gnbnegright=0\n",
    "gnbposright=0\n",
    "for i in range(len(testlabel)):\n",
    "    if testlabel[i]==rfresult[i] and testlabel[i]==1:\n",
    "        rfposright+=1\n",
    "    if testlabel[i]==rfresult[i] and testlabel[i]==0:\n",
    "        rfnegright+=1\n",
    "    if testlabel[i]==abresult[i] and testlabel[i]==1:\n",
    "        abposright+=1\n",
    "    if testlabel[i]==abresult[i] and testlabel[i]==0:\n",
    "        abnegright+=1\n",
    "    if testlabel[i]==gbresult[i] and testlabel[i]==1:\n",
    "        gbposright+=1\n",
    "    if testlabel[i]==gbresult[i] and testlabel[i]==0:\n",
    "        gbnegright+=1\n",
    "    if testlabel[i]==svmresult[i] and testlabel[i]==1:\n",
    "        svmposright+=1\n",
    "    if testlabel[i]==svmresult[i] and testlabel[i]==0:\n",
    "        svmnegright+=1\n",
    "    if testlabel[i]==bgresult[i] and testlabel[i]==1:\n",
    "        bgposright+=1\n",
    "    if testlabel[i]==bgresult[i] and testlabel[i]==0:\n",
    "        bgnegright+=1\n",
    "    if testlabel[i]==gnbresult[i] and testlabel[i]==1:\n",
    "        gnbposright+=1\n",
    "    if testlabel[i]==gnbresult[i] and testlabel[i]==0:\n",
    "        gnbnegright+=1       \n",
    "rftotalacu=(rfposright+rfnegright)/151\n",
    "rfposright/=72\n",
    "rfnegright/=79\n",
    "abtotalacu=(abposright+abnegright)/151\n",
    "abposright/=72\n",
    "abnegright/=79\n",
    "gbtotalacu=(gbposright+gbnegright)/151\n",
    "gbposright/=72\n",
    "gbnegright/=79\n",
    "svmtotalacu=(svmposright+svmnegright)/151\n",
    "svmposright/=72\n",
    "svmnegright/=79\n",
    "bgtotalacu=(bgposright+bgnegright)/151\n",
    "bgposright/=72\n",
    "bgnegright/=79\n",
    "gnbtotalacu=(gnbposright+gnbnegright)/151\n",
    "gnbposright/=72\n",
    "gnbnegright/=79\n",
    "\n",
    "print(\"Based on intensity with autoencoder---------------\")\n",
    "\n",
    "print(\"random forest:\",rfposright,rfnegright,rftotalacu)\n",
    "print(\"adaboost:\",abposright,abnegright,abtotalacu)\n",
    "print(\"gradient boost:\",gbposright,gbnegright,gbtotalacu)\n",
    "print(\"svm:\",svmposright,svmnegright,svmtotalacu)\n",
    "print(\"bagging:\",bgposright,bgnegright,bgtotalacu)\n",
    "print(\"gaussian naive bayes:\",gnbposright,gnbnegright,gnbtotalacu)        \n",
    "        \n",
    "    \n",
    "# rfindex=[i for i,x in enumerate(rfresult) if x==0]\n",
    "# abindex=[i for i,x in enumerate(abresult) if x==0]\n",
    "# gbindex=[i for i,x in enumerate(gbresult) if x==0]\n",
    "# print(\"Based on intensity---------------\")\n",
    "# print(\"random forest:\",rfresult,rfindex)\n",
    "# print(\"adaboost:\",abresult,abindex)\n",
    "# print(\"gradient boost:\",gbresult,gbindex)\n",
    "#print(\"svm:\",svmclf.predict(intensity))\n",
    "#print(\"bagging:\",bgclf.predict(intensity))\n",
    "#print(\"gaussian naive bayes:\",gnbclf.predict(intensity))\n",
    "#print(\"SGD:\",sgdclf.predict(intensity))\n",
    "\n",
    "# rfacu=rfclf.score(encoded_testdata,testlabel)\n",
    "# abacu=abclf.score(encoded_testdata,testlabel)\n",
    "# gbacu=gbclf.score(encoded_testdata,testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose one case and get its intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'unsigned short', 'endian': 'little', 'keyvaluepairs': {}, 'space': 'left-posterior-superior', 'encoding': 'gzip', 'dimension': 3, 'kinds': ['domain', 'domain', 'domain'], 'sizes': [640, 416, 120], 'space directions': [['0.46875', '0', '0'], ['0', '0.46875', '0'], ['0', '0', '1.600006103515625']], 'space origin': ['-185.93734741210935', '-105.98280334472663', '-106.57722473144517']}\n",
      "['-185.93734741210935', '-105.98280334472663', '-106.57722473144517']\n",
      "2\n",
      "1\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nrrd\n",
    "import glob\n",
    "import math\n",
    "needlespath='case064vtk-auto.save'\n",
    "nrrdpath = os.path.expanduser(\"~\")+'/DATA/Case  064/NRRD/Manual_CY/16 AX space trial 1.6 SLICE COVER TEMPLATE.nrrd'\n",
    "thistopath= 'needleintensity/totalhisto.save'\n",
    "tmeanpath= 'needleintensity/totalhisto.save'\n",
    "\n",
    "def print_full(x):\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    print(x)\n",
    "    np.set_printoptions(threshold=1000)\n",
    "    \n",
    "def str2float(list):\n",
    "    newlist=[]\n",
    "    n=list.ndim\n",
    "    print(n)\n",
    "    if n>1:       \n",
    "        for j in range(len(list)):\n",
    "            temp=[float(i) for i in list[j]]\n",
    "            newlist.append(temp)\n",
    "    else:\n",
    "        newlist=[float(i) for i in list]\n",
    "    return newlist    \n",
    "\n",
    "def gettransmat(spacing,origin):\n",
    "    spacing.append([0.,0.,0.])\n",
    "    origin.append(1.)\n",
    "    spacing=np.array(spacing)\n",
    "    origin=np.array(origin)\n",
    "    transmat=np.linalg.inv(np.c_[ spacing,origin ])\n",
    "    return transmat\n",
    "\n",
    "def transcoord(data,transmat):\n",
    "    data=data.transpose()\n",
    "    data=np.append(data,np.array([[1. for i in range(5000)]]),axis=0)\n",
    "#    print(data,data.shape)\n",
    "    \n",
    "    newdata=np.dot(transmat,data)\n",
    "    newdata=np.rint(newdata)\n",
    "    newdata=newdata.astype(int)\n",
    "#    print(newdata,newdata.shape)\n",
    "    \n",
    "    return newdata\n",
    "\n",
    "def getintensity(imgneedles):\n",
    "    meangray=[]\n",
    "    intensity=[]\n",
    "    rng=[-1,0,1]\n",
    "    for i in range(len(imgneedles)):\n",
    "        mean=0\n",
    "        graylevel=[]\n",
    "        for j in range(5000):\n",
    "            temp=2000\n",
    "            for x1 in rng:\n",
    "                for x2 in rng:\n",
    "                    if nrrddata[0][imgneedles[i][0][j]+x1][imgneedles[i][1][j]+x2][imgneedles[i][2][j]]<temp:\n",
    "                        temp=nrrddata[0][imgneedles[i][0][j]+x1][imgneedles[i][1][j]+x2][imgneedles[i][2][j]]\n",
    "            gray=temp\n",
    "            graylevel.append(gray)\n",
    "            mean+=gray\n",
    "        mean/=5000\n",
    "        meangray.append(mean)\n",
    "        intensity.append(graylevel)\n",
    "    return np.array(meangray),np.array(intensity)\n",
    "\n",
    "def gethistogram(intensity):\n",
    "    totalhis=[]\n",
    "    for i in range(len(intensity)):\n",
    "        upbound=10\n",
    "        histogram=[]\n",
    "        while upbound<155:\n",
    "            num=sum(i < upbound and i>=(upbound-10) for i in intensity[i])/5000\n",
    "            histogram.append(num)\n",
    "            upbound+=10\n",
    "        num=1-sum(histogram)\n",
    "        histogram.append(num)\n",
    "        totalhis.append(histogram)\n",
    "        totalhis2=np.around(totalhis,2)\n",
    "    return totalhis2   \n",
    "\n",
    "def getsegmean(intensity):\n",
    "    totalsegmean=[]\n",
    "    for i in range(len(intensity)):\n",
    "        upbound=500\n",
    "        segmean=[]\n",
    "        while upbound<=5000:\n",
    "            num=np.mean(intensity[i][upbound-500:upbound])\n",
    "            segmean.append(num)\n",
    "            upbound+=500\n",
    "        totalsegmean.append(segmean)\n",
    "        totalsegmean2=np.around(totalsegmean,2)\n",
    "    return totalsegmean2 \n",
    "\n",
    "def sortseq(needles):\n",
    "    newseq=[]\n",
    "    for i in range(len(needles)):\n",
    "        temp=needles[i][needles[i][:,2].argsort()]\n",
    "        #print(temp)\n",
    "        newseq.append(temp)\n",
    "    return newseq    \n",
    "totalhisto=np.load(thistopath)\n",
    "averhisto=np.mean(totalhisto,axis=0)\n",
    "needles = np.load(needlespath)\n",
    "needles[:,:,:2]=needles[:,:,:2]*(-1.)\n",
    "needles=sortseq(needles)\n",
    "needles=np.array(needles)\n",
    "nrrddata = nrrd.read(nrrdpath)\n",
    "print(nrrddata[1])\n",
    "print(nrrddata[1]['space origin'])  \n",
    "nrrdspacing=str2float(np.array(nrrddata[1]['space directions']))\n",
    "nrrdsporig=str2float(np.array(nrrddata[1]['space origin'])) \n",
    "\n",
    "transmat=gettransmat(nrrdspacing,nrrdsporig)\n",
    "\n",
    "imgneedles=[]\n",
    "for i in range(len(needles)):\n",
    "    temp=transcoord(needles[i],transmat)\n",
    "    imgneedles.append(temp)\n",
    "    \n",
    "mean,intensity=getintensity(imgneedles)\n",
    "histo=gethistogram(intensity)\n",
    "segmean=getsegmean(intensity)\n",
    "\n",
    "for i in range(len(intensity)):\n",
    "    for j in range(5000):\n",
    "        if intensity[i][j]>255:\n",
    "            intensity[i][j]=255\n",
    "intensity=intensity.astype('float32') / 255.\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict real case and give result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True [ 0.78  0.06  0.05  0.04  0.02  0.02  0.01  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "1 True [ 0.81  0.08  0.03  0.02  0.02  0.01  0.02  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "2 False [ 0.23  0.04  0.28  0.11  0.06  0.05  0.03  0.03  0.04  0.03  0.04  0.02\n",
      "  0.01  0.    0.01  0.02]\n",
      "3 True [ 0.72  0.1   0.05  0.04  0.03  0.01  0.02  0.01  0.01  0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "4 True [ 0.74  0.08  0.04  0.05  0.02  0.02  0.01  0.02  0.01  0.01  0.01  0.01\n",
      "  0.    0.    0.    0.  ]\n",
      "5 True [ 0.72  0.08  0.05  0.04  0.03  0.03  0.01  0.01  0.    0.01  0.01  0.    0.\n",
      "  0.    0.    0.  ]\n",
      "6 True [ 0.65  0.12  0.05  0.06  0.04  0.02  0.02  0.02  0.01  0.01  0.01  0.    0.\n",
      "  0.    0.    0.01]\n",
      "7 True [ 0.73  0.09  0.05  0.03  0.01  0.01  0.01  0.01  0.02  0.01  0.    0.01\n",
      "  0.    0.01  0.    0.02]\n",
      "8 False [ 0.6   0.13  0.09  0.06  0.04  0.03  0.02  0.01  0.01  0.    0.01  0.    0.\n",
      "  0.    0.    0.  ]\n",
      "9 False [ 0.54  0.12  0.08  0.06  0.06  0.04  0.05  0.03  0.02  0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "10 False [ 0.57  0.08  0.07  0.08  0.04  0.04  0.04  0.02  0.02  0.02  0.01  0.01\n",
      "  0.01  0.    0.    0.  ]\n",
      "11 False [ 0.6   0.07  0.03  0.01  0.03  0.01  0.01  0.02  0.01  0.    0.02  0.02\n",
      "  0.    0.01  0.03  0.11]\n",
      "12 True [ 0.73  0.1   0.06  0.03  0.03  0.01  0.01  0.    0.    0.01  0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "13 True [ 0.74  0.07  0.04  0.03  0.01  0.02  0.02  0.01  0.01  0.01  0.01  0.\n",
      "  0.01  0.    0.    0.01]\n",
      "14 True [ 0.61  0.15  0.08  0.05  0.05  0.02  0.02  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "15 True [ 0.81  0.06  0.04  0.03  0.02  0.01  0.02  0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "16 True [ 0.81  0.06  0.05  0.03  0.01  0.01  0.01  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "17 True [ 0.93  0.02  0.01  0.01  0.01  0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "18 False [ 0.15  0.03  0.02  0.03  0.03  0.05  0.05  0.04  0.07  0.05  0.1   0.08\n",
      "  0.04  0.08  0.07  0.12]\n",
      "19 False [ 0.5   0.02  0.02  0.04  0.02  0.07  0.07  0.05  0.05  0.06  0.03  0.01\n",
      "  0.01  0.01  0.    0.04]\n",
      "20 False [ 0.52  0.08  0.07  0.05  0.05  0.04  0.04  0.02  0.03  0.03  0.02  0.02\n",
      "  0.01  0.01  0.    0.01]\n",
      "21 True [ 0.89  0.03  0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.01  0.01  0.    0.03]\n",
      "22 True [ 0.84  0.07  0.04  0.01  0.01  0.    0.01  0.01  0.    0.01  0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "23 True [ 0.77  0.1   0.07  0.01  0.02  0.01  0.01  0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "24 True [ 0.7   0.07  0.06  0.05  0.04  0.02  0.02  0.01  0.01  0.01  0.01  0.    0.\n",
      "  0.    0.    0.  ]\n",
      "25 True [ 0.7   0.13  0.09  0.02  0.03  0.01  0.01  0.01  0.01  0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "26 True [ 0.63  0.09  0.07  0.03  0.05  0.03  0.01  0.01  0.01  0.01  0.01  0.01\n",
      "  0.01  0.01  0.    0.01]\n",
      "27 True [ 0.82  0.07  0.04  0.02  0.02  0.02  0.01  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "28 False [ 0.47  0.07  0.05  0.05  0.05  0.07  0.06  0.04  0.03  0.03  0.02  0.01\n",
      "  0.01  0.02  0.01  0.02]\n",
      "29 False [ 0.63  0.09  0.05  0.04  0.04  0.04  0.05  0.02  0.01  0.01  0.    0.01\n",
      "  0.    0.    0.    0.  ]\n",
      "30 True [ 0.62  0.14  0.12  0.03  0.03  0.01  0.01  0.01  0.    0.    0.01  0.    0.\n",
      "  0.    0.    0.  ]\n",
      "31 True [ 0.77  0.09  0.06  0.03  0.02  0.01  0.01  0.01  0.    0.    0.    0.    0.\n",
      "  0.    0.    0.  ]\n",
      "32 True [ 0.65  0.07  0.06  0.03  0.04  0.03  0.02  0.02  0.02  0.01  0.01  0.01\n",
      "  0.    0.    0.    0.  ]\n",
      "33 False [ 0.52  0.07  0.05  0.03  0.05  0.05  0.04  0.05  0.03  0.03  0.03  0.02\n",
      "  0.01  0.01  0.01  0.  ]\n",
      "34 True [ 0.82  0.03  0.02  0.03  0.03  0.02  0.01  0.01  0.01  0.01  0.    0.    0.\n",
      "  0.    0.   -0.  ]\n",
      "35 False [ 0.58  0.06  0.05  0.04  0.04  0.05  0.05  0.02  0.02  0.01  0.02  0.01\n",
      "  0.01  0.    0.01  0.04]\n",
      "36 True [ 0.63  0.16  0.08  0.06  0.02  0.01  0.01  0.    0.01  0.    0.    0.    0.\n",
      "  0.    0.   -0.  ]\n",
      "37 False [ 0.43  0.07  0.06  0.07  0.06  0.07  0.06  0.07  0.04  0.03  0.01  0.\n",
      "  0.01  0.    0.   -0.  ]\n",
      "38 False [ 0.51  0.05  0.07  0.03  0.02  0.03  0.02  0.05  0.03  0.04  0.02  0.03\n",
      "  0.03  0.03  0.    0.03]\n",
      "39 True [ 0.56  0.13  0.06  0.05  0.07  0.05  0.03  0.01  0.01  0.01  0.01  0.    0.\n",
      "  0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "encoded_testdata = encoder.predict(intensity)\n",
    "print(encoded_traindata.shape)\n",
    "#predict testdata\n",
    "rfresult=rfclf.predict(encoded_testdata)\n",
    "abresult=abclf.predict(encoded_testdata)\n",
    "gbresult=gbclf.predict(encoded_testdata)\n",
    "svmresult=svmclf.predict(encoded_testdata)\n",
    "bgresult=bgclf.predict(encoded_testdata)\n",
    "gnbresult=gnbclf.predict(encoded_testdata)\n",
    "\n",
    "rfindex=[i for i,x in enumerate(rfresult) if x==0]\n",
    "\n",
    "for i in range(40):\n",
    "    print(i,i not in rfindex,histo[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "build a neural network for training encoded intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.4833 - acc: 0.7488     \n",
      "Epoch 2/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.4010 - acc: 0.8056     \n",
      "Epoch 3/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3760 - acc: 0.8187     \n",
      "Epoch 4/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3438 - acc: 0.8450     \n",
      "Epoch 5/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3478 - acc: 0.8431     \n",
      "Epoch 6/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3259 - acc: 0.8506     \n",
      "Epoch 7/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3097 - acc: 0.8631     \n",
      "Epoch 8/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3138 - acc: 0.8600     \n",
      "Epoch 9/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3081 - acc: 0.8662     \n",
      "Epoch 10/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3063 - acc: 0.8625     \n",
      "Epoch 11/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3008 - acc: 0.8719     \n",
      "Epoch 12/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3010 - acc: 0.8669     \n",
      "Epoch 13/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3052 - acc: 0.8619     \n",
      "Epoch 14/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3082 - acc: 0.8744     \n",
      "Epoch 15/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2967 - acc: 0.8656     \n",
      "Epoch 16/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2983 - acc: 0.8700     \n",
      "Epoch 17/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2984 - acc: 0.8669     \n",
      "Epoch 18/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2883 - acc: 0.8675     \n",
      "Epoch 19/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3054 - acc: 0.8738     \n",
      "Epoch 20/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3057 - acc: 0.8650     \n",
      "Epoch 21/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2934 - acc: 0.8700     \n",
      "Epoch 22/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2975 - acc: 0.8706     \n",
      "Epoch 23/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3004 - acc: 0.8631     \n",
      "Epoch 24/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3017 - acc: 0.8731     \n",
      "Epoch 25/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2874 - acc: 0.8706     \n",
      "Epoch 26/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2931 - acc: 0.8744     \n",
      "Epoch 27/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3018 - acc: 0.8650     \n",
      "Epoch 28/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2990 - acc: 0.8713     \n",
      "Epoch 29/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2869 - acc: 0.8706     \n",
      "Epoch 30/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3015 - acc: 0.8794     \n",
      "Epoch 31/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2892 - acc: 0.8806     \n",
      "Epoch 32/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3032 - acc: 0.8762     \n",
      "Epoch 33/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2945 - acc: 0.8762     \n",
      "Epoch 34/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3025 - acc: 0.8738     \n",
      "Epoch 35/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2912 - acc: 0.8781     \n",
      "Epoch 36/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3014 - acc: 0.8725     \n",
      "Epoch 37/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2973 - acc: 0.8731     \n",
      "Epoch 38/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2854 - acc: 0.8738     \n",
      "Epoch 39/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3002 - acc: 0.8744     \n",
      "Epoch 40/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3015 - acc: 0.8694     \n",
      "Epoch 41/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3006 - acc: 0.8669     \n",
      "Epoch 42/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2926 - acc: 0.8825     \n",
      "Epoch 43/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2935 - acc: 0.8812     \n",
      "Epoch 44/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2897 - acc: 0.8681     \n",
      "Epoch 45/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2928 - acc: 0.8787     \n",
      "Epoch 46/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2875 - acc: 0.8812     \n",
      "Epoch 47/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2947 - acc: 0.8781     \n",
      "Epoch 48/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2927 - acc: 0.8756     \n",
      "Epoch 49/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2878 - acc: 0.8700     \n",
      "Epoch 50/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3056 - acc: 0.8844     \n",
      "Epoch 51/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2942 - acc: 0.8869     \n",
      "Epoch 52/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2923 - acc: 0.8781     \n",
      "Epoch 53/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2876 - acc: 0.8819     \n",
      "Epoch 54/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2929 - acc: 0.8769     \n",
      "Epoch 55/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2903 - acc: 0.8806     \n",
      "Epoch 56/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2884 - acc: 0.8794     \n",
      "Epoch 57/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2969 - acc: 0.8781     \n",
      "Epoch 58/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2831 - acc: 0.8800     \n",
      "Epoch 59/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2849 - acc: 0.8806     \n",
      "Epoch 60/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2927 - acc: 0.8775     \n",
      "Epoch 61/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2829 - acc: 0.8750     \n",
      "Epoch 62/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2921 - acc: 0.8787     \n",
      "Epoch 63/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2881 - acc: 0.8825     \n",
      "Epoch 64/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2922 - acc: 0.8775     \n",
      "Epoch 65/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2981 - acc: 0.8800     \n",
      "Epoch 66/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2912 - acc: 0.8869     \n",
      "Epoch 67/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2862 - acc: 0.8869     \n",
      "Epoch 68/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2838 - acc: 0.8825     \n",
      "Epoch 69/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2934 - acc: 0.8787     \n",
      "Epoch 70/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2918 - acc: 0.8800     \n",
      "Epoch 71/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2915 - acc: 0.8831     \n",
      "Epoch 72/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2777 - acc: 0.8838     \n",
      "Epoch 73/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2984 - acc: 0.8750     \n",
      "Epoch 74/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2873 - acc: 0.8781     \n",
      "Epoch 75/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2922 - acc: 0.8869     \n",
      "Epoch 76/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2886 - acc: 0.8863     \n",
      "Epoch 77/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2856 - acc: 0.8806     \n",
      "Epoch 78/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2876 - acc: 0.8869     \n",
      "Epoch 79/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2912 - acc: 0.8825     \n",
      "Epoch 80/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2868 - acc: 0.8869     \n",
      "Epoch 81/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2861 - acc: 0.8800     \n",
      "Epoch 82/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2933 - acc: 0.8806     \n",
      "Epoch 83/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2928 - acc: 0.8781     \n",
      "Epoch 84/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2776 - acc: 0.8856     \n",
      "Epoch 85/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2796 - acc: 0.8925     \n",
      "Epoch 86/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2873 - acc: 0.8875     \n",
      "Epoch 87/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2828 - acc: 0.8875     \n",
      "Epoch 88/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2937 - acc: 0.8844     \n",
      "Epoch 89/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2910 - acc: 0.8863     \n",
      "Epoch 90/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2941 - acc: 0.8819     \n",
      "Epoch 91/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2840 - acc: 0.8850     \n",
      "Epoch 92/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2875 - acc: 0.8812     \n",
      "Epoch 93/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2809 - acc: 0.8888     \n",
      "Epoch 94/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2841 - acc: 0.8812     \n",
      "Epoch 95/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2837 - acc: 0.8844     \n",
      "Epoch 96/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2954 - acc: 0.8750     \n",
      "Epoch 97/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2938 - acc: 0.8838     \n",
      "Epoch 98/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2807 - acc: 0.8844     \n",
      "Epoch 99/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2859 - acc: 0.8894     \n",
      "Epoch 100/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2863 - acc: 0.8806     \n",
      "Epoch 101/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2899 - acc: 0.8775     \n",
      "Epoch 102/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2790 - acc: 0.8856     \n",
      "Epoch 103/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2815 - acc: 0.8869     \n",
      "Epoch 104/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2863 - acc: 0.8900     \n",
      "Epoch 105/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3064 - acc: 0.8825     \n",
      "Epoch 106/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2972 - acc: 0.8844     \n",
      "Epoch 107/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2943 - acc: 0.8850     \n",
      "Epoch 108/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2892 - acc: 0.8819     \n",
      "Epoch 109/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2844 - acc: 0.8869     \n",
      "Epoch 110/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2845 - acc: 0.8806     \n",
      "Epoch 111/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2906 - acc: 0.8800     \n",
      "Epoch 112/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2718 - acc: 0.8937     \n",
      "Epoch 113/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2946 - acc: 0.8856     \n",
      "Epoch 114/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2730 - acc: 0.8894     \n",
      "Epoch 115/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2864 - acc: 0.8881     \n",
      "Epoch 116/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2832 - acc: 0.8850     \n",
      "Epoch 117/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.3026 - acc: 0.8850     \n",
      "Epoch 118/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2779 - acc: 0.8894     \n",
      "Epoch 119/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2852 - acc: 0.8844     \n",
      "Epoch 120/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2821 - acc: 0.8881     \n",
      "Epoch 121/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2903 - acc: 0.8844     \n",
      "Epoch 122/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2891 - acc: 0.8856     \n",
      "Epoch 123/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2963 - acc: 0.8869     \n",
      "Epoch 124/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2821 - acc: 0.8869     \n",
      "Epoch 125/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2869 - acc: 0.8863     \n",
      "Epoch 126/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2793 - acc: 0.8937     \n",
      "Epoch 127/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2835 - acc: 0.8919     \n",
      "Epoch 128/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2862 - acc: 0.8856     \n",
      "Epoch 129/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2841 - acc: 0.8875     \n",
      "Epoch 130/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2738 - acc: 0.8931     \n",
      "Epoch 131/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2774 - acc: 0.8863     \n",
      "Epoch 132/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2854 - acc: 0.8863     \n",
      "Epoch 133/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2777 - acc: 0.8875     \n",
      "Epoch 134/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2832 - acc: 0.8881     \n",
      "Epoch 135/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2786 - acc: 0.8925     \n",
      "Epoch 136/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2758 - acc: 0.8881     \n",
      "Epoch 137/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2831 - acc: 0.8844     \n",
      "Epoch 138/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2710 - acc: 0.8869     \n",
      "Epoch 139/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2824 - acc: 0.8863     \n",
      "Epoch 140/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2705 - acc: 0.8875     \n",
      "Epoch 141/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2804 - acc: 0.8881     \n",
      "Epoch 142/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2935 - acc: 0.8819     \n",
      "Epoch 143/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2871 - acc: 0.8881     \n",
      "Epoch 144/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2760 - acc: 0.8838     \n",
      "Epoch 145/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2784 - acc: 0.8869     \n",
      "Epoch 146/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2828 - acc: 0.8881     \n",
      "Epoch 147/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2729 - acc: 0.8881     \n",
      "Epoch 148/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2747 - acc: 0.8881     \n",
      "Epoch 149/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2839 - acc: 0.8825     \n",
      "Epoch 150/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2664 - acc: 0.8944     \n",
      "Epoch 151/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2960 - acc: 0.8931     \n",
      "Epoch 152/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2782 - acc: 0.8850     \n",
      "Epoch 153/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2788 - acc: 0.8844     \n",
      "Epoch 154/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2813 - acc: 0.8925     \n",
      "Epoch 155/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2868 - acc: 0.8838     \n",
      "Epoch 156/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2922 - acc: 0.8888     \n",
      "Epoch 157/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2762 - acc: 0.8831     \n",
      "Epoch 158/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2836 - acc: 0.8863     \n",
      "Epoch 159/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2780 - acc: 0.8900     \n",
      "Epoch 160/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2822 - acc: 0.8831     \n",
      "Epoch 161/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2798 - acc: 0.8869     \n",
      "Epoch 162/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2718 - acc: 0.8919     \n",
      "Epoch 163/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2764 - acc: 0.8850     \n",
      "Epoch 164/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2776 - acc: 0.8912     \n",
      "Epoch 165/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2867 - acc: 0.8912     \n",
      "Epoch 166/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2794 - acc: 0.8850     \n",
      "Epoch 167/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2879 - acc: 0.8881     \n",
      "Epoch 168/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2895 - acc: 0.8838     \n",
      "Epoch 169/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2794 - acc: 0.8869     \n",
      "Epoch 170/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2863 - acc: 0.8787     \n",
      "Epoch 171/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2905 - acc: 0.8881     \n",
      "Epoch 172/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2815 - acc: 0.8844     \n",
      "Epoch 173/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2877 - acc: 0.8888     \n",
      "Epoch 174/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2728 - acc: 0.8900     \n",
      "Epoch 175/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2783 - acc: 0.8900     \n",
      "Epoch 176/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2768 - acc: 0.8906     \n",
      "Epoch 177/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2882 - acc: 0.8869     \n",
      "Epoch 178/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2729 - acc: 0.8919     \n",
      "Epoch 179/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2764 - acc: 0.8919     \n",
      "Epoch 180/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2806 - acc: 0.8869     \n",
      "Epoch 181/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2795 - acc: 0.8869     \n",
      "Epoch 182/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2847 - acc: 0.8812     \n",
      "Epoch 183/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2783 - acc: 0.8850     \n",
      "Epoch 184/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2832 - acc: 0.8906     \n",
      "Epoch 185/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2636 - acc: 0.8906     \n",
      "Epoch 186/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2814 - acc: 0.8894     \n",
      "Epoch 187/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2753 - acc: 0.8850     \n",
      "Epoch 188/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2840 - acc: 0.8869     \n",
      "Epoch 189/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2702 - acc: 0.8981     \n",
      "Epoch 190/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2879 - acc: 0.8806     \n",
      "Epoch 191/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2763 - acc: 0.8869     \n",
      "Epoch 192/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2670 - acc: 0.8937     \n",
      "Epoch 193/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2884 - acc: 0.8831     \n",
      "Epoch 194/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2790 - acc: 0.8931     \n",
      "Epoch 195/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2800 - acc: 0.8794     \n",
      "Epoch 196/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2657 - acc: 0.8869     \n",
      "Epoch 197/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2780 - acc: 0.8863     \n",
      "Epoch 198/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2915 - acc: 0.8888     \n",
      "Epoch 199/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2718 - acc: 0.8894     \n",
      "Epoch 200/200\n",
      "1600/1600 [==============================] - 0s - loss: 0.2820 - acc: 0.8856     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8a33fa6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=32, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(encoded_traindata, trainlabel,\n",
    "          nb_epoch=200,\n",
    "          batch_size=16)\n",
    "\n",
    "# result=model.predict(intensity)\n",
    "# print(result)\n",
    "#score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/151 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18818191364900955, 0.9403973525723085]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(encoded_testdata, testlabel, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666 0.8607594936708861 0.8874172185430463\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(encoded_testdata)\n",
    "result=[1 if x>0.5 else 0 for x in result]\n",
    "nnposright=0\n",
    "nnnegright=0\n",
    "for i in range(len(testlabel)):\n",
    "    if testlabel[i]==result[i] and testlabel[i]==1:\n",
    "        nnposright+=1\n",
    "    if testlabel[i]==result[i] and testlabel[i]==0:\n",
    "        nnnegright+=1\n",
    "totalacu=(nnposright+nnnegright)/151\n",
    "nnposright/=72\n",
    "nnnegright/=79\n",
    "print(nnposright,nnnegright,totalacu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "build a neural network for training intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1600/1600 [==============================] - 2s - loss: 1.0659 - acc: 0.5694     \n",
      "Epoch 2/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.6513 - acc: 0.6950     \n",
      "Epoch 3/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.4879 - acc: 0.7950     \n",
      "Epoch 4/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.4402 - acc: 0.8144     \n",
      "Epoch 5/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.4104 - acc: 0.8444     \n",
      "Epoch 6/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3834 - acc: 0.8538     \n",
      "Epoch 7/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3763 - acc: 0.8731     \n",
      "Epoch 8/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3427 - acc: 0.8881     \n",
      "Epoch 9/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3193 - acc: 0.9044     \n",
      "Epoch 10/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3519 - acc: 0.9094     \n",
      "Epoch 11/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3168 - acc: 0.9169     \n",
      "Epoch 12/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3430 - acc: 0.9144     \n",
      "Epoch 13/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3064 - acc: 0.9206     \n",
      "Epoch 14/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3192 - acc: 0.9175     \n",
      "Epoch 15/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2465 - acc: 0.9256     \n",
      "Epoch 16/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2980 - acc: 0.9269     \n",
      "Epoch 17/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3023 - acc: 0.9281     \n",
      "Epoch 18/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3168 - acc: 0.9244     \n",
      "Epoch 19/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2542 - acc: 0.9313     \n",
      "Epoch 20/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2922 - acc: 0.9231     \n",
      "Epoch 21/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3156 - acc: 0.9394     \n",
      "Epoch 22/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3209 - acc: 0.9262     \n",
      "Epoch 23/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3108 - acc: 0.9338     \n",
      "Epoch 24/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2765 - acc: 0.9319     \n",
      "Epoch 25/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2758 - acc: 0.9325     \n",
      "Epoch 26/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2699 - acc: 0.9363     \n",
      "Epoch 27/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3142 - acc: 0.9287     \n",
      "Epoch 28/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2389 - acc: 0.9381     \n",
      "Epoch 29/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2950 - acc: 0.9338     \n",
      "Epoch 30/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3129 - acc: 0.9369     \n",
      "Epoch 31/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2796 - acc: 0.9338     \n",
      "Epoch 32/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2385 - acc: 0.9419     \n",
      "Epoch 33/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2683 - acc: 0.9375     \n",
      "Epoch 34/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2365 - acc: 0.9494     \n",
      "Epoch 35/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.3055 - acc: 0.9406     \n",
      "Epoch 36/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2173 - acc: 0.9444     \n",
      "Epoch 37/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2343 - acc: 0.9375     \n",
      "Epoch 38/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2824 - acc: 0.9425     \n",
      "Epoch 39/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2866 - acc: 0.9387     \n",
      "Epoch 40/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1936 - acc: 0.9525     \n",
      "Epoch 41/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2171 - acc: 0.9437     \n",
      "Epoch 42/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2510 - acc: 0.9456     \n",
      "Epoch 43/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2117 - acc: 0.9475     \n",
      "Epoch 44/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2333 - acc: 0.9494     \n",
      "Epoch 45/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2248 - acc: 0.9513     \n",
      "Epoch 46/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2367 - acc: 0.9475     \n",
      "Epoch 47/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2185 - acc: 0.9513     \n",
      "Epoch 48/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1956 - acc: 0.9469     \n",
      "Epoch 49/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1981 - acc: 0.9550     \n",
      "Epoch 50/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2306 - acc: 0.9562     \n",
      "Epoch 51/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1428 - acc: 0.9581     \n",
      "Epoch 52/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2136 - acc: 0.9619     \n",
      "Epoch 53/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2155 - acc: 0.9575     \n",
      "Epoch 54/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1919 - acc: 0.9619     \n",
      "Epoch 55/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2020 - acc: 0.9556     \n",
      "Epoch 56/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1872 - acc: 0.9613     \n",
      "Epoch 57/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1408 - acc: 0.9631     \n",
      "Epoch 58/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1998 - acc: 0.9663     \n",
      "Epoch 59/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2085 - acc: 0.9587     \n",
      "Epoch 60/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1327 - acc: 0.9637     \n",
      "Epoch 61/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1743 - acc: 0.9619     \n",
      "Epoch 62/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1601 - acc: 0.9594     \n",
      "Epoch 63/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1494 - acc: 0.9656     \n",
      "Epoch 64/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1582 - acc: 0.9706     \n",
      "Epoch 65/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1604 - acc: 0.9675     \n",
      "Epoch 66/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1384 - acc: 0.9694     \n",
      "Epoch 67/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1321 - acc: 0.9706     \n",
      "Epoch 68/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1382 - acc: 0.9700     \n",
      "Epoch 69/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1306 - acc: 0.9681     \n",
      "Epoch 70/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1235 - acc: 0.9681     \n",
      "Epoch 71/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1179 - acc: 0.9669     \n",
      "Epoch 72/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1234 - acc: 0.9756     \n",
      "Epoch 73/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1450 - acc: 0.9712     \n",
      "Epoch 74/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1520 - acc: 0.9644     \n",
      "Epoch 75/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1038 - acc: 0.9775     \n",
      "Epoch 76/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1024 - acc: 0.9750     \n",
      "Epoch 77/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1060 - acc: 0.9756     \n",
      "Epoch 78/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1246 - acc: 0.9750     \n",
      "Epoch 79/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1268 - acc: 0.9750     \n",
      "Epoch 80/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1423 - acc: 0.9762     \n",
      "Epoch 81/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1273 - acc: 0.9738     \n",
      "Epoch 82/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1258 - acc: 0.9725     \n",
      "Epoch 83/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1375 - acc: 0.9712     \n",
      "Epoch 84/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1731 - acc: 0.9731     \n",
      "Epoch 85/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.2077 - acc: 0.9694     \n",
      "Epoch 86/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0936 - acc: 0.9738     \n",
      "Epoch 87/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1238 - acc: 0.9813     \n",
      "Epoch 88/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1343 - acc: 0.9762     \n",
      "Epoch 89/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1124 - acc: 0.9750     \n",
      "Epoch 90/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1171 - acc: 0.9769     \n",
      "Epoch 91/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1229 - acc: 0.9738     \n",
      "Epoch 92/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1371 - acc: 0.9744     \n",
      "Epoch 93/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0945 - acc: 0.9794     \n",
      "Epoch 94/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1061 - acc: 0.9788     \n",
      "Epoch 95/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1114 - acc: 0.9781     \n",
      "Epoch 96/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1146 - acc: 0.9800     \n",
      "Epoch 97/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1303 - acc: 0.9781     \n",
      "Epoch 98/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1664 - acc: 0.9769     \n",
      "Epoch 99/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1095 - acc: 0.9762     \n",
      "Epoch 100/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1150 - acc: 0.9794     \n",
      "Epoch 101/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1193 - acc: 0.9806     \n",
      "Epoch 102/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0885 - acc: 0.9819     \n",
      "Epoch 103/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1139 - acc: 0.9788     \n",
      "Epoch 104/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1172 - acc: 0.9850     \n",
      "Epoch 105/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1786 - acc: 0.9769     \n",
      "Epoch 106/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0879 - acc: 0.9838     \n",
      "Epoch 107/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1336 - acc: 0.9775     \n",
      "Epoch 108/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1236 - acc: 0.9831     \n",
      "Epoch 109/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1125 - acc: 0.9844     \n",
      "Epoch 110/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1346 - acc: 0.9806     \n",
      "Epoch 111/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1115 - acc: 0.9794     \n",
      "Epoch 112/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0684 - acc: 0.9825     \n",
      "Epoch 113/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1387 - acc: 0.9794     \n",
      "Epoch 114/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1432 - acc: 0.9775     \n",
      "Epoch 115/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1190 - acc: 0.9844     \n",
      "Epoch 116/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1195 - acc: 0.9819     \n",
      "Epoch 117/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1091 - acc: 0.9869     \n",
      "Epoch 118/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1159 - acc: 0.9850     \n",
      "Epoch 119/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1404 - acc: 0.9800     \n",
      "Epoch 120/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1295 - acc: 0.9862     \n",
      "Epoch 121/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1084 - acc: 0.9781     \n",
      "Epoch 122/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1172 - acc: 0.9856     \n",
      "Epoch 123/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1042 - acc: 0.9831     \n",
      "Epoch 124/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0937 - acc: 0.9869     \n",
      "Epoch 125/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1400 - acc: 0.9819     \n",
      "Epoch 126/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1354 - acc: 0.9819     \n",
      "Epoch 127/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1494 - acc: 0.9850     \n",
      "Epoch 128/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0945 - acc: 0.9788     \n",
      "Epoch 129/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1064 - acc: 0.9850     \n",
      "Epoch 130/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1050 - acc: 0.9844     \n",
      "Epoch 131/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0984 - acc: 0.9856     \n",
      "Epoch 132/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1078 - acc: 0.9844     \n",
      "Epoch 133/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1458 - acc: 0.9856     \n",
      "Epoch 134/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1018 - acc: 0.9881     \n",
      "Epoch 135/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1191 - acc: 0.9887     \n",
      "Epoch 136/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0865 - acc: 0.9875     \n",
      "Epoch 137/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0963 - acc: 0.9850     \n",
      "Epoch 138/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1007 - acc: 0.9869     \n",
      "Epoch 139/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1238 - acc: 0.9844     \n",
      "Epoch 140/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0559 - acc: 0.9906     \n",
      "Epoch 141/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1154 - acc: 0.9862     \n",
      "Epoch 142/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1303 - acc: 0.9894     \n",
      "Epoch 143/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1286 - acc: 0.9856     \n",
      "Epoch 144/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1057 - acc: 0.9887     \n",
      "Epoch 145/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1149 - acc: 0.9875     \n",
      "Epoch 146/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1339 - acc: 0.9850     \n",
      "Epoch 147/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1366 - acc: 0.9850     \n",
      "Epoch 148/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0742 - acc: 0.9912     \n",
      "Epoch 149/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0649 - acc: 0.9925     \n",
      "Epoch 150/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1079 - acc: 0.9894     \n",
      "Epoch 151/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1301 - acc: 0.9881     \n",
      "Epoch 152/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0490 - acc: 0.9944     \n",
      "Epoch 153/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1179 - acc: 0.9906     \n",
      "Epoch 154/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1558 - acc: 0.9825     \n",
      "Epoch 155/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1127 - acc: 0.9856     \n",
      "Epoch 156/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1151 - acc: 0.9875     \n",
      "Epoch 157/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0994 - acc: 0.9856     \n",
      "Epoch 158/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0894 - acc: 0.9875     \n",
      "Epoch 159/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0638 - acc: 0.9906     \n",
      "Epoch 160/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0657 - acc: 0.9919     \n",
      "Epoch 161/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0772 - acc: 0.9912     \n",
      "Epoch 162/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1353 - acc: 0.9862     \n",
      "Epoch 163/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1064 - acc: 0.9875     \n",
      "Epoch 164/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1325 - acc: 0.9838     \n",
      "Epoch 165/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1029 - acc: 0.9906     \n",
      "Epoch 166/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0754 - acc: 0.9887     \n",
      "Epoch 167/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1224 - acc: 0.9844     \n",
      "Epoch 168/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0912 - acc: 0.9862     \n",
      "Epoch 169/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0839 - acc: 0.9931     \n",
      "Epoch 170/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1088 - acc: 0.9887     \n",
      "Epoch 171/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0888 - acc: 0.9894     \n",
      "Epoch 172/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0854 - acc: 0.9887     \n",
      "Epoch 173/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1477 - acc: 0.9831     \n",
      "Epoch 174/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0838 - acc: 0.9887     \n",
      "Epoch 175/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1259 - acc: 0.9887     \n",
      "Epoch 176/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1271 - acc: 0.9862     \n",
      "Epoch 177/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0807 - acc: 0.9906     \n",
      "Epoch 178/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1130 - acc: 0.9894     \n",
      "Epoch 179/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0675 - acc: 0.9919     \n",
      "Epoch 180/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0819 - acc: 0.9912     \n",
      "Epoch 181/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1587 - acc: 0.9850     \n",
      "Epoch 182/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0721 - acc: 0.9912     \n",
      "Epoch 183/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1341 - acc: 0.9887     \n",
      "Epoch 184/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0809 - acc: 0.9912     \n",
      "Epoch 185/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1111 - acc: 0.9887     \n",
      "Epoch 186/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1121 - acc: 0.9881     \n",
      "Epoch 187/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0787 - acc: 0.9894     \n",
      "Epoch 188/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1133 - acc: 0.9887     \n",
      "Epoch 189/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1322 - acc: 0.9831     \n",
      "Epoch 190/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.0698 - acc: 0.9944     \n",
      "Epoch 191/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1377 - acc: 0.9875     \n",
      "Epoch 192/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1285 - acc: 0.9881     \n",
      "Epoch 193/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1787 - acc: 0.9856     \n",
      "Epoch 194/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1202 - acc: 0.9887     \n",
      "Epoch 195/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1052 - acc: 0.9912     \n",
      "Epoch 196/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1537 - acc: 0.9875     \n",
      "Epoch 197/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1196 - acc: 0.9869     \n",
      "Epoch 198/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1163 - acc: 0.9881     \n",
      "Epoch 199/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1317 - acc: 0.9887     \n",
      "Epoch 200/200\n",
      "1600/1600 [==============================] - 2s - loss: 0.1143 - acc: 0.9881     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8a2f6fdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=5000, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(traindata, trainlabel,\n",
    "          nb_epoch=200,\n",
    "          batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/151 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0145814118795835, 0.86092715705467371]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testdata, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "0.9166666666666666 0.7721518987341772 0.8410596026490066\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(testdata)\n",
    "result=[1 if x>0.5 else 0 for x in result]\n",
    "nnposright=0\n",
    "nnnegright=0\n",
    "for i in range(len(testlabel)):\n",
    "    if testlabel[i]==result[i] and testlabel[i]==1:\n",
    "        nnposright+=1\n",
    "    if testlabel[i]==result[i] and testlabel[i]==0:\n",
    "        nnnegright+=1\n",
    "totalacu=(nnposright+nnnegright)/151\n",
    "nnposright/=72\n",
    "nnnegright/=79\n",
    "print(result)\n",
    "print(nnposright,nnnegright,totalacu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
